{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Corpora and Treebanks\n",
    "\n",
    "In the field of Natural Langugage Processing, a sample of real world text is referred to as a *Corpus* (plural *Corpora*).\n",
    "\n",
    "Possibly the most famous and widely used corpus is the **Brown Corpus** (https://en.wikipedia.org/wiki/Brown_Corpus) which was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "\n",
    "A *Treebank* is an parsed and annotated Corpus that records the syntactic or semantic sentence structure of the content (i.e. typically each sentence and probably each word in the corpus).   The first large-scale treebank was The **Penn Treebank**, created in 1992 at the University of Pennsylvania. \n",
    "\n",
    "There are many corpora and treebanks distributed with the NLTK toolkit, listed in the NLTK Book here: http://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Exploring a Corpus ##\n",
    "\n",
    "The Corpus are not installed by default - if you want to use one of the standard NLTK corpora, use the NLTK downloader as shown in the example below.\n",
    "\n",
    "**Example** - Download the Reuters Corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Various methods can be applied against a loaded corpus to perform actions such as extract the words from corpus, identify the files that make up a corpus, list category definitions in a corpus and extract text filtered by category.\n",
    "\n",
    "A list of common methods is provided in Chapter 2 of the [NLTK Book](http://www.nltk.org/book/ch02.html) as follows:  \n",
    "<table align=\"left\">\n",
    "<tr><td><b>Example</b></td>   <td align=\"left\"><b>Description</b></td></tr>\n",
    "<tr><td>fileids()</td>   <td align=\"left\">the files of the corpus</td></tr>\n",
    "<tr><td>fileids([categories])</td>   <td align=\"left\">the files of the corpus corresponding to these categories</td></tr>\n",
    "<tr><td>categories()</td>   <td align=\"left\">the categories of the corpus</td></tr>\n",
    "<tr><td>categories([fileids])</td>   <td align=\"left\">the categories of the corpus corresponding to these files</td></tr>\n",
    "<tr><td>raw()</td>   <td align=\"left\">the raw content of the corpus</td></tr>\n",
    "<tr><td>raw(fileids=[f1,f2,f3])</td>   <td align=\"left\">the raw content of the specified files</td></tr>\n",
    "<tr><td>raw(categories=[c1,c2])</td>   <td align=\"left\">the raw content of the specified categories</td></tr>\n",
    "<tr><td>words()</td>   <td align=\"left\">the words of the whole corpus</td></tr>\n",
    "<tr><td>words(fileids=[f1,f2,f3])</td>   <td align=\"left\">the words of the specified fileids</td></tr>\n",
    "<tr><td>words(categories=[c1,c2])</td>   <td align=\"left\">the words of the specified categories</td></tr>\n",
    "<tr><td>sents()</td>   <td align=\"left\">the sentences of the whole corpus</td></tr>\n",
    "<tr><td>sents(fileids=[f1,f2,f3])</td>   <td align=\"left\">the sentences of the specified fileids</td></tr>\n",
    "<tr><td>sents(categories=[c1,c2])</td>   <td align=\"left\">the sentences of the specified categories</td></tr>\n",
    "<tr><td>abspath(fileid)</td>   <td align=\"left\">the location of the given file on disk</td></tr>\n",
    "<tr><td>encoding(fileid)</td>   <td align=\"left\">the encoding of the file (if known)</td></tr>\n",
    "<tr><td>open(fileid)</td>   <td align=\"left\">open a stream for reading the given corpus file</td></tr>\n",
    "<tr><td>root</td>   <td align=\"left\">if the path to the root of locally installed corpus</td></tr>\n",
    "<tr><td>readme()</td>   <td align=\"left\">the contents of the README file of the corpus</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download some addition Corpora from NLTK####\n",
    "These are for use later on in this set of Python Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')   # Punkt Tokenizer Model\n",
    "nltk.download('averaged_perceptron_tagger')  # Part-of-Speech Tokeniser\n",
    "nltk.download(\"stopwords\") # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples with the Reuters Corpus ####\n",
    "\n",
    "The original Reuters Corpus was compiled from newswire articles from 1987.  In 1990 the documents were made available by Reuters and CGI *for research purposes only*.  One of the central people involved in developing this corpus was David Lewis, and he has published more information and background here: http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt\n",
    "\n",
    "The original corpus contained 22,173 documents and was known as the \"Reuters-22173\" corpus.  This was then pared down by removing duplicated and became known as the Reuters-21578 collection.  \n",
    "\n",
    "The Reuters Corpus is released with the following requirement: *\"If you publish results based on this data set, please acknowledge its use, refer to the data set by the name 'Reuters-21578, Distribution 1.0', and inform your readers of the current location of the data set.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories, Files and Words ###\n",
    "\n",
    "Load the corpus and list any categories defined in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "categories = reuters.categories()\n",
    "print(\"Number of Categories:\",len(categories))\n",
    "print(categories[0:9],categories[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words can be extracted with the `words()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = reuters.words()\n",
    "print(\"number of words\", len(words) )\n",
    "print(\"first 10 words:\", words[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Text by Category ##\n",
    "\n",
    "Get all news article words classified under \"trade\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract at a sepecific category\n",
    "tradeWords = reuters.words(categories = 'trade')\n",
    "len(tradeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords and Punctuation ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes a couple of minutes to run\n",
    "tradeWords = [w for w in tradeWords if w.lower() not in stopwords.words('english') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeWords = [w for w in tradeWords if w not in string.punctuation]\n",
    "punctCombo = [c+\"\\\"\" for c in string.punctuation ]+ [\"\\\"\"+c for c in string.punctuation ]\n",
    "tradeWords = [w for w in tradeWords if w not in punctCombo]\n",
    "len(tradeWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency Distribution ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(tradeWords)\n",
    "fdist.plot(20, cumulative=False)\n",
    "# (installed Ghostscript on my PC to get this working in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, frequency in fdist.most_common(10):\n",
    "    print(word, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Grams###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biTradeWords = nltk.bigrams(tradeWords)\n",
    "biFdist = nltk.FreqDist(biTradeWords)\n",
    "biFdist.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Penn Treebank ##\n",
    "The NLTK data package includes a 10% sample of the Penn Treebank.  This can be used for exploring the concepts of Part-of-Speech tagging and the concept of sentence word classifications with grouping and heirarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = treebank.words()\n",
    "tagged = treebank.tagged_words()\n",
    "print(type(tagged))\n",
    "print(\"Word Count\", len(words))\n",
    "print(\"Tagged words sample: \",tagged[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This treebank allows us to extract specific parsed sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = treebank.parsed_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.core.display.display(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Investigation ##\n",
    "\n",
    "+ Import the Brown Corpus  \n",
    "+ List the categories  and count the number of words by category\n",
    "+ Extract the first sentence from the corpus\n",
    "+ Compare the average sentence length between Religion, News and Humor categories\n",
    "+ Identify the top-20 words and bigrams in sentences categoried as \"religion\" and \"news\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
