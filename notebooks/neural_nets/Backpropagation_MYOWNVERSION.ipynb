{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exTaPMM2RM0h"
   },
   "source": [
    "![img](https://drive.google.com/uc?id=1TYfSkbrOKiAKwXF_sAF_XyMnk4ShfPJP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%run \"BackpropModule.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vShFnQDbRM0m"
   },
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BggrIUnkRM0p"
   },
   "outputs": [],
   "source": [
    "# Here is the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "    \n",
    "\n",
    "# This is the cost function of a neural network with respect to a training set.\n",
    "def cost(x, y) :\n",
    "    \"\"\"\n",
    "    network_function(x)[-1] is the last layer a3\n",
    "    division by x.size... isn't that always equal to one in this scenario ?? why divide ??\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZCewSzoRM0s"
   },
   "outputs": [],
   "source": [
    "# LAYER 3\n",
    "\n",
    "def J_W3 (y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x)\n",
    "    \n",
    "    # dC/da3,\n",
    "    J = 2 * (a3 - y) # layer 3 has 2 nodes: [2 x 1] \n",
    "    \n",
    "    # da3/dz3\n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    # dz3/dW3\n",
    "    J = J @ a2.T # [2 x 1] x [1 x 2] = [2 x 2]\n",
    "    \n",
    "    # then divide by the number of training examples, for the average over all training examples.\n",
    "    J = J / m \n",
    "    print('shape J_W3: ', J.shape)\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "def J_b3 (y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x)\n",
    "    \n",
    "    # dC/da3\n",
    "    J = 2 * (a3 - y) # [2 x 1] \n",
    "    \n",
    "    # da3/dz3\n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    # dz3/db3\n",
    "    J = J \n",
    "    \n",
    "    # sum over all training examples however.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / m\n",
    "    print('shape J_b3: ', J.shape)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GxRfERJRM0v"
   },
   "outputs": [],
   "source": [
    "# LAYER 2\n",
    "\n",
    "def J_W2 (y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x) \n",
    "    \n",
    "    # dC/da3\n",
    "    J = 2 * (a3 - y) # [2 x 1] \n",
    "    \n",
    "    # da3/dz3\n",
    "    J = J * d_sigma(z3) # [2 x 1] \n",
    "    \n",
    "    # dz3/da2\n",
    "    # layer 3 has two nodes thus W3 = [2 x n2]\n",
    "    J = (J.T @ W3).T # [1 x 2] x [2 x n2] = [1 x n2]\n",
    "    \n",
    "    # da2/dz2\n",
    "    J = J * d_sigma(z2)\n",
    "    \n",
    "    # dz2/dW2\n",
    "    J = J @ a1.T # [1 x n2] x [n2 x 1]  = [1 x 1]\n",
    "    \n",
    "    # then divide by the number of training examples, for the average over all training examples.\n",
    "    J = J / m \n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "def J_b2 (y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x)\n",
    "    \n",
    "    # dC/da3\n",
    "    J = 2 * (a3 - y) # [2 x 1] \n",
    "    \n",
    "    # da3/dz3\n",
    "    J = J * d_sigma(z3) # [2 x 1] \n",
    "    \n",
    "    # dz3/da2\n",
    "    # layer 3 has two nodes thus W3 = [2 x n2]\n",
    "    J = (J.T @ W3).T # [1 x 2] x [2 x n2] = [1 x n2]\n",
    "    \n",
    "    # da2/dz2\n",
    "    J = J * d_sigma(z2)\n",
    "    \n",
    "    # dz2/db2\n",
    "    J = J \n",
    "    \n",
    "    # sum over all training examples\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJYIRrpRRM0y"
   },
   "outputs": [],
   "source": [
    "# LAYER 1\n",
    "\n",
    "def J_W1 (y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x) \n",
    "    \n",
    "    # dC/da3\n",
    "    J = 2 * (a3 - y) # [2 x 1] \n",
    "    \n",
    "    ##### da3/dz3 #####################################\n",
    "    J = J * d_sigma(z3) # [2 x 1] \n",
    "    \n",
    "    # dz3/da2\n",
    "    # layer 3 has 2 nodes: W3 = [2 x n2]\n",
    "    J = (J.T @ W3).T # [1 x 2] x [2 x n2] = [1 x n2].T = [n2 x 1]\n",
    "    ###################################################\n",
    "    \n",
    "    ##### da2/z2 ######################################\n",
    "    J = J * d_sigma(z2) # [n2 x 1]\n",
    "    \n",
    "    # dz2/da1\n",
    "    # layer 2 has n2 nodes thus W2 = [n2 x n1]\n",
    "    J = (J.T @ W2).T # [1 x n2] x [n2 x n1] = [1 x n1].T = [n1 x 1]\n",
    "    ###################################################\n",
    "    \n",
    "    # da1/dz1\n",
    "    J = J * d_sigma(z1) # [n1 x 1]\n",
    "    \n",
    "    # dz1/dW1\n",
    "    # a0 shape is [m x 1]\n",
    "    \n",
    "    J = J @ a0.T # [n1 x 1] x [1 x m]  = [n1 x m]\n",
    "    \n",
    "    # then divide by the number of training examples, for the average over all training examples.\n",
    "    J = J / m \n",
    "    \n",
    "    return J\n",
    "  \n",
    "\n",
    "def J_b1 (y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = feed_forward(x)\n",
    "\n",
    "    # dC/da3\n",
    "    J = 2 * (a3 - y) # [2 x 1] \n",
    "    \n",
    "    ##### da3/dz3 ######################################\n",
    "    J = J * d_sigma(z3) # [2 x 1] \n",
    "    \n",
    "    # dz3/da2\n",
    "    # layer 3 has two nodes: W3 = [2 x n2]\n",
    "    J = (J.T @ W3).T # [1 x 2] x [2 x n2] = [1 x n2].T = [n2 x 1]\n",
    "    ####################################################\n",
    "    \n",
    "    ##### da2/z2 #######################################\n",
    "    J = J * d_sigma(z2) # [n2 x 1]\n",
    "    \n",
    "    # dz2/da1\n",
    "    # layer 2 has n2 nodes thus W2 = [n2 x n1]\n",
    "    J = (J.T @ W2).T # [1 x n2] x [n2 x n1] = [1 x n1].T = [n1 x 1]\n",
    "    ####################################################\n",
    "    \n",
    "    # da1/dz1\n",
    "    J = J * d_sigma(z1) # [n1 x 1]\n",
    "    \n",
    "    # dz1/db1\n",
    "    J = J \n",
    "    \n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1XNQfZ3AJeb"
   },
   "outputs": [],
   "source": [
    "global W1, W2, W3, b1, b2, b3\n",
    "def backprop(x, y, iterations=1, aggression=3.5, noise=1) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    \n",
    "    while iterations>=0:\n",
    "        # compute\n",
    "        j_W1 = J_W1(y)\n",
    "        j_W2 = J_W2(y)\n",
    "        j_W3 = J_W3(y)\n",
    "        j_b1 = J_b1(y)\n",
    "        j_b2 = J_b2(y)\n",
    "        j_b3 = J_b3(y)\n",
    "\n",
    "        # update\n",
    "        W1 = W1 - j_W1 \n",
    "        W2 = W2 - j_W2 \n",
    "        W3 = W3 - j_W3 \n",
    "        b1 = b1 - j_b1 \n",
    "        b2 = b2 - j_b2 \n",
    "        b3 = b3 - j_b3 \n",
    "\n",
    "#         if (iterations%100==0) :\n",
    "#             nf = network_function(x)[-1]\n",
    "    \n",
    "        iterations -= 1\n",
    "\n",
    "    a3 = feed_forward(x)[-1]\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOY_aJir2Wbi"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=123)\n",
    "\n",
    "def training_data (M = 5) :\n",
    "    x = np.arange(0,1,1/M) # nbr btw 0 and 1 in 1/N steps\n",
    "    \n",
    "    y = np.array([0,1])\n",
    "    \n",
    "    x = np.reshape(x, (M, 1))\n",
    "    print('x shape: ', x.shape)\n",
    "    \n",
    "    y = np.reshape(y, (2, 1))\n",
    "     \n",
    "    m = x.size\n",
    "    \n",
    "    return x, y, m\n",
    "  \n",
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "def feed_forward(a0) :\n",
    "    \n",
    "    # [3 x m] x [m x 1] + [3 x 1]\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1) # [3 x 1]\n",
    "    \n",
    "    # [2 x 3] x [3 x 1] + [2 x 3]\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2) # [2 x 1]\n",
    "    \n",
    "    # [2 x 2] x [2 x 1] + [2 x 1]\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3) # [2 x 1]\n",
    "    \n",
    "    print('shape a1', a1.shape)\n",
    "    print('shape a2', a2.shape)\n",
    "    print('shape a3', a3.shape)\n",
    "    \n",
    "    return a0, z1, a1, z2, a2, z3, a3\n",
    "  \n",
    "  \n",
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "# n1 = 6, n2 = 7\n",
    "def reset_network (n0, n1 = 3, n2 = 2, n3=2, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, n0) / 2 ## 3 x 1\n",
    "    W2 = random.randn(n2, n1) / 2 ## 2 x 3\n",
    "    W3 = random.randn(n3, n2) / 2 ## 2 x 2\n",
    "    print('shape W1', W1.shape)\n",
    "    print('shape W2', W2.shape)\n",
    "    print('shape W3', W3.shape)\n",
    "    \n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(n3, 1) / 2\n",
    "    print('shape b1', b1.shape)\n",
    "    print('shape b2', b2.shape)\n",
    "    print('shape b3', b3.shape)\n",
    "\n",
    "    \n",
    "# test\n",
    "# global W1, W2, W3, b1, b2, b3\n",
    "# x, y, m = training_data()\n",
    "# reset_network(n0=m)\n",
    "# a0, z1, a1, z2, a2, z3, a3 = feed_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1560855334045,
     "user": {
      "displayName": "Raoul Biagioni",
      "photoUrl": "",
      "userId": "07128102106560353825"
     },
     "user_tz": -60
    },
    "id": "fl8rJuxGRM03",
    "outputId": "cf61420a-7e23-4815-88d6-412ddddfd509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape W1 (3, 5)\n",
      "shape W2 (2, 3)\n",
      "shape W3 (2, 2)\n",
      "shape b1 (3, 1)\n",
      "shape b2 (2, 1)\n",
      "shape b3 (2, 1)\n",
      "x shape:  (5, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape J_W3:  (2, 2)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape J_b3:  (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape J_W3:  (2, 2)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "shape J_b3:  (2, 1)\n",
      "shape a1 (3, 1)\n",
      "shape a2 (2, 1)\n",
      "shape a3 (2, 1)\n",
      "[[0.49492989]\n",
      " [0.24723845]]\n"
     ]
    }
   ],
   "source": [
    "reset_network(n0=5)\n",
    "x, y, m = training_data()\n",
    "a3 = backprop(x, y, iterations=1, aggression=7, noise=1)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpM1GmQ0RM07"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "neu2cNwxFx4A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Backpropagation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
