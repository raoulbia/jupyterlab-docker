{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Classification Using Machine Learning\n",
    "\n",
    "To classify sentences into one of several Categories, various Machine Learning and Statistical techniques can be used.  This note focuses on using a Supervised Learning mechanism, developing a model trained on a set of pre-classified sentences.\n",
    "\n",
    "A particular approach is taken, avoiding the use of counting specific types of words (i.e. \"question words\") in the features and instead considers Part-Of-Speech patterns in a sentence.  For a full model this could be combined with word-count and other features.\n",
    "\n",
    "**Notebook Process Flow**\n",
    "1. Load Data\n",
    "2. Extract Features\n",
    "3. Build a Model against the Training Data Set and Validate against Test Set\n",
    "4. Test the Model against a different Data Set\n",
    "\n",
    "**Dependencies**  \n",
    "\n",
    "This Python notebook is dependent on the set of classified sentences in **[sentences.csv](https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/sentences.csv)** (or an equivlent set of data) to run.  In addition to this, another data-set called **[pythonFAQ.csv](https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/pythonFAQ.csv)** is used to test the model.  These need to be downloaded in advance and then the path to these files needs to be set manually in the notebook to correctly reference where they are located.\n",
    "\n",
    "In order to build a classification model, we need to extract some features, and this is the bulk of effort for the task as detailed in this note.  The Python Sci-Kit Learn library contains a comprehensive pre-packaged machine learning algorithms that can then be used with data-set.\n",
    "\n",
    "The approach for extracting features is demonstrated in part with in-line code in the notebook, but the full set of functionality is wrapped in a bespoke module **[features.py](https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/features.py)** that needs to be downloaded in advance and then the path to this file needs to be set manually in the notebook to correctly reference where it is located.  The output of running various functions in features.py is saved in the file **[featuresDump.csv](https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/featuresDump.csv)** which also needs to be downloaded in advance and stored in the same location as the other CSV files.\n",
    "\n",
    "To download these files, either clone the whole GitHub Repo https://github.com/edbullen/NLPBot , or download each file individually:\n",
    "```\n",
    "wget https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/sentences.csv\n",
    "wget https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/pythonFAQ.csv\n",
    "wget https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/features.py\n",
    "wget https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/featuresDump.csv\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data ##\n",
    "\n",
    "First, load some pre-classified data from a CSV file called \"sentences.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load 100 sentences with a classification Q/S/C\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CODE_LOC = 'D://SugarSync/git/Python/NLPBot/'   # !! Modify to path to \"features.py\" folder lcoation\n",
    "DATA_LOC = 'D://SugarSync/git/Python/NLPBot/analysis/sentences.csv'  # !! Modify this to the CSV data location\n",
    "\n",
    "sentences = pd.read_csv(filepath_or_buffer = DATA_LOC)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering - A Non-Standard, Bespoke Approach ##\n",
    "\n",
    "Chapter 6 of the NLTK Book has a great deal of background and worked examples for classifying text using machine learning algorithms such as Naive Bayes Classifiers.   A different bespoke approach involving home-grown feature engineering and a scikit-learn Random Forest model is outlined in this note.\n",
    "\n",
    "The code snippet below is an example of taking a sentence and extracting sets of *POS-tag Triples* from it.  We can use this approach for building up features from a sentence by counting occurances of triple-patterns (or other POS-tag patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some patterns of PoS sequences\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "list_of_triple_strings = []  # triple sequence of PoS tags\n",
    "sentence = \"Can a dog see in colour?\"\n",
    "\n",
    "sentenceParsed = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(sentenceParsed)\n",
    "pos = [ i[1] for i in pos_tags ]\n",
    "print(\"Words mapped to Part of Speech Tags:\",pos_tags)\n",
    "print(\"PoS Tags:\", pos)\n",
    "\n",
    "n = len(pos)\n",
    "for i in range(0,n-3):\n",
    "    t = \"-\".join(pos[i:i+3]) # pull out 3 list item from counter, convert to string\n",
    "    list_of_triple_strings.append(t)\n",
    "    \n",
    "print(\"sequences of triples:\", list_of_triple_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features ###\n",
    "After pre-processing the sentences (using the approach above) we can get a set of triples for Questions, Chat, Statements.  There will be a lot of intersection, but hopefully some clear patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `features.py` Features Generator ###\n",
    "This is a custom Python module to extract features from a sentence, written for this ChatBot demo.\n",
    "\n",
    "`features.py` is located here: https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/features.py\n",
    "    \n",
    "Just\n",
    "```\n",
    "import features\n",
    "```\n",
    "and call \n",
    "```\n",
    "features = features_dict(id,sentence, c)\n",
    "```\n",
    "\n",
    "to extract a dictionary of features for the given sentence.  \n",
    "\n",
    "+ The \"id\" can be any arbirtary ID value - it just get s passed in and passout as an ID identifier in the resultant dictionary.\n",
    "+ The \"c\" value can also be any arbitrary value representing the Class label - the idea is to supply an appropriate label so that the dict that is passed back has all the necessary information in it.\n",
    "\n",
    "The actual features that are generated and the logic behind how this is done is all hard-coded in features.py (it is not paramaterised - a potential enhancement that could be added)\n",
    "\n",
    "#### `features.py` POS Triples Extract ####\n",
    "\n",
    "The features.py module includes a function  \n",
    "```\n",
    "get_triples(pos)\n",
    "```  \n",
    "which returns a string of the form `\"POS-POS-POS\"`  where \"POS\" is a Part-Of-Speech tag.\n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(CODE_LOC)  # set search path to code cloned from GitHub\n",
    "import features            # bespoke \"feature engineering\" module\n",
    "\n",
    "sentence = \"Can a dog see in colour?\"\n",
    "\n",
    "sentence = features.strip_sentence(sentence)\n",
    "print(sentence)\n",
    "pos = features.get_pos(sentence)\n",
    "triples = features.get_triples(pos)\n",
    "\n",
    "print(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process for Identifying Candidate Features - Analysis in SQL ####\n",
    "The objective is to identify candidate PoS sequences that signify a liklihood of a Statement / Question / Chat sentence.\n",
    "\n",
    "Approach: dump all triples for each sentence with sentence-type label (\"S\"/\"Q\"/\"C\") recorded for each item into a SQL database.\n",
    "\n",
    "**Count all triples**\n",
    "```sql\n",
    "SELECT count(*) FROM triples;  \n",
    "> 360\n",
    "```  \n",
    "\n",
    "**Break-down by label type**\n",
    "```sql\n",
    "SELECT count(triple),label \n",
    "FROM triples\n",
    "GROUP by label;  \n",
    "\n",
    " count(triple) label\n",
    ">          37  C\n",
    ">         145  Q\n",
    ">         178  S\n",
    "```   \n",
    "\n",
    "**Common occuring triple sequences by label type ** \n",
    "```sql\n",
    "SELECT label, triple, occurences\n",
    "FROM\n",
    "    (SELECT triples.label label, triples.triple triple, count(triples.triple) occurences\n",
    "    FROM triples,\n",
    "        (select triple, count(triple) occurences\n",
    "         from triples\n",
    "         group by triple) counts\n",
    "     WHERE counts.occurences > 2\n",
    "     AND triples.triple = counts.triple\n",
    "     GROUP BY triples.triple, triples.label\n",
    "     ORDER BY 2,1\n",
    "     ) triples_by_label\n",
    "WHERE occurences > 1\n",
    " ;\n",
    "```\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> CD-VB-VBN </font></td> <td align=\"left\">5</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\">DT-JJ-NN </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">DT-NN-NN</td> <td bgcolor=\"lightgrey\" align=\"left\">3</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">DT-NN-NN</td> <td bgcolor=\"lightgrey\" align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> DT-NN-VBZ </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\">DT-NNP-IN</td> <td align=\"left\">2</td></tr>\n",
    "<tr bgcolor=\"lightgrey\"><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">DT-NNP-NN</td> <td bgcolor=\"lightgrey\" align=\"left\">4</td></tr>\n",
    "<tr bgcolor=\"lightgrey\"><td>S</td> <td align=\"left\">DT-NNP-NN</td> <td align=\"left\">5</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"><font color=\"green\"> DT-NNP-NNP </font> </td> <td align=\"left\">4</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> IN-DT-NN </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">IN-DT-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">4</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">IN-DT-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\">IN-NN-NNS </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\">MD-PRP-VB </font> </td> <td align=\"left\">5</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\">MD-VB-CD </font> </td> <td align=\"left\">4</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\">MD-VB-VBN </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> NN-IN-DT </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">NN-NN-IN</td> <td bgcolor=\"lightgrey\" align=\"left\">2</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">NN-NN-IN</td> <td bgcolor=\"lightgrey\" align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> NNP-IN-NNP </font> </td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> NNP-NNP-NNP </font> </td> <td align=\"left\">14</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">NNP-NNP-VBZ</td> <td bgcolor=\"lightgrey\"  align=\"left\">2</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">NNP-NNP-VBZ</td> <td bgcolor=\"lightgrey\" align=\"left\">4</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> NNP-VBZ-DT </font></td> <td align=\"left\">8</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> NNP-VBZ-NNP </font></td> <td align=\"left\">5</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> NNS-IN-DT </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> PRP-VB-PRP </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> PRP-WP-NNP </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> VB-CD-VB </font> </td> <td align=\"left\">4</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> VB-PRP-WP </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> VB-VBN-IN </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>S</td> <td align=\"left\"> <font color=\"green\"> VBZ-DT-JJ </font></td> <td align=\"left\">3</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> VBZ-DT-NN </font> </td> <td align=\"left\">7</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">VBZ-DT-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">2</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">VBZ-DT-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">5</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">Q</td> <td bgcolor=\"lightgrey\" align=\"left\">VBZ-NNP-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">3</td></tr>\n",
    "<tr><td bgcolor=\"lightgrey\">S</td> <td bgcolor=\"lightgrey\" align=\"left\">VBZ-NNP-NNP</td> <td bgcolor=\"lightgrey\" align=\"left\">5</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> WP-VBZ-DT </font> </td> <td align=\"left\">6</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> WP-VBZ-NNP </font> </td> <td align=\"left\">9</td></tr>\n",
    "<tr><td>Q</td> <td align=\"left\"> <font color=\"red\"> WRB-MD-VB </font> </td> <td align=\"left\">4</td></tr>\n",
    "<tr><td>C</td> <td align=\"left\">WRB-VBP-PRP</td> <td align=\"left\">3</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bespoke Features Generator Example - Get a Python Dictionary of Features ####\n",
    "sentences = [\"Can a dog see in colour?\",\n",
    "             \"Hey, How's it going?\",\n",
    "             \"Oracle 12.2 will be released for on-premises users on 15 March 2017\",\n",
    "             \"When will Oracle 12 be released\"]\n",
    "id = 1\n",
    "for s in sentences:\n",
    "    features_dict = features.features_dict(str(id),s)\n",
    "    features_string,header = features.get_string(str(id),s)\n",
    "    print(features_dict)\n",
    "    #print(features_string)\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "With this approach we can bulk generate some numeric data-features generated from a CSV file of sentences. If each sentence has a unique ID and we have a classifier label (S/Q/C) for each row observation, we can now try to build a ML classification model and assess it's effectiveness.\n",
    "\n",
    "The script `featuresDump.py` processes a raw `sentences.csv` file with the `features.py` utility and dumps out a file in a format as listed below:\n",
    "\n",
    " ```\n",
    " id, wordCount, stemmedCount, stemmedEndNN, CD, NN, NNP, NNPS, NNS, PRP, VBG, VBZ, startTuple0, endTuple0, endTuple1, endTuple2, verbBeforeNoun, qMark, qVerbCombo, qTripleScore, sTripleScore, class  \n",
    " 44d8a78d2ca66b1b, 7, 5, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, S  \n",
    " a9133770c79b2c43, 7, 4, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 2, C  \n",
    " 246cf41a55627762, 5, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, Q  \n",
    " 53ac5757399632e8, 6, 4, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, S  \n",
    " 78e580bde0b4396e, 6, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, Q  \n",
    "...  \n",
    "...  \n",
    "...  \n",
    " 036d7e8be25c3108, 4, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, Q  \n",
    " b2dd2ca708214c2a, 6, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 4, 0, Q  \n",
    " 73ebcc1f94f38ddf, 3, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, C  \n",
    " 617c60a010967c8a, 4, 3, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, C  \n",
    " ecef7fa7fcb25f20, 9, 6, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, S  \n",
    " 16fb4f28223d22a9, 7, 5, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, Q  \n",
    " 7fea2d04212f8039, 8, 5, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, S  \n",
    " 3df9464caeef89a4, 13, 7, 0, 0, 3, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 3, S  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Build a Machine Learning Model  ##\n",
    "\n",
    "In this section we load a features CSV file called **`featuresDump.csv`** into a Pandas data-frame.  The data was generated with `features.py` reading in the `sentences.csv` file as described in the previous section.  The featuresDump.csv data is then used to train a Random Forest model to predict whether a sentence is **C**hat, **S**tatement or **Q**uestion.\n",
    "\n",
    "The `featuresDump.csv` file can be downloaded from here: https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/featuresDump.csv\n",
    "\n",
    "#### Load the Data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "FNAME = 'D://SugarSync/git/Python/NLPBot/analysis/featuresDump.csv' # !! Modify this to the CSV data location\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer = FNAME, )   \n",
    "print(str(len(df)), \"rows loaded\")\n",
    "\n",
    "# Strip any leading spaces from col names\n",
    "df.columns = df.columns[:].str.strip()\n",
    "df['class'] = df['class'].map(lambda x: x.strip())\n",
    "\n",
    "width = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into Test and Training Sets ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into test and training (is_train: True / False col)\n",
    "np.random.seed(seed=1)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "print(str(len(train)), \" rows split into training set,\", str(len(test)), \"split into test set.\")\n",
    "\n",
    "features = df.columns[1:width-1]  #remove the first ID col and last col=classifier\n",
    "print(\"FEATURES = {}\".format(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Model with the Training Data-Set ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an RF Model for \"class\" given features\n",
    "clf = RandomForestClassifier(n_jobs=2, n_estimators = 100)\n",
    "clf.fit(train[features], train['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Predictions from the Test Data-Set ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predict against test set\n",
    "preds = clf.predict(test[features])\n",
    "predout = pd.DataFrame({ 'id' : test['id'], 'predicted' : preds, 'actual' : test['class'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Validation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-check accuracy ##\n",
    "print(pd.crosstab(test['class'], preds, rownames=['actual'], colnames=['preds']))\n",
    "print(\"\\n\",pd.crosstab(test['class'], preds, rownames=['actual']\n",
    "                       , colnames=['preds']).apply(lambda r: round(r/r.sum()*100,2), axis=1) )\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\n\\nAccuracy Score: \", round(accuracy_score(test['class'], preds),3) ) # https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flaws in the Approach and Further Validation ####\n",
    "\n",
    "The accuracy appears pretty good, but the approach taken probably means we have over-fitted the feature selection.  In the next section we try out the model on a completely different data-set, taken from the Python FaQ at https://docs.python.org/3/faq/general.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model Against the Python FAQ ##\n",
    "\n",
    "#### Generate Features using the Features Generator ####\n",
    "A prepared CSV containing Statements and Questions from the Python FAQ site (https://docs.python.org/3/faq/general.html) can be downloaded from here: https://github.com/edbullen/NLPBot/releases/download/SupportingFiles/pythonFAQ.csv\n",
    "\n",
    "Some random chat statements have been added to the file as well - EG \"What do you reckon?\" and \"yeah, whatever\".\n",
    "\n",
    "#### Load Sentence Data and Generate Features ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in some pre-formated FAQ data in a CSV\n",
    "FNAME = 'D://SugarSync/git/Python/NLPBot/analysis/pythonFAQ.csv' # !! Modify this to the CSV data location\n",
    "\n",
    "import csv\n",
    "import hashlib \n",
    "\n",
    "import features\n",
    "\n",
    "fin = open(FNAME, 'rt')\n",
    "reader = csv.reader(fin)\n",
    "\n",
    "keys = [\"id\",\n",
    "\"wordCount\",\n",
    "\"stemmedCount\",\n",
    "\"stemmedEndNN\",\n",
    "\"CD\",\n",
    "\"NN\",\n",
    "\"NNP\",\n",
    "\"NNPS\",\n",
    "\"NNS\",\n",
    "\"PRP\",\n",
    "\"VBG\",\n",
    "\"VBZ\",\n",
    "\"startTuple0\",\n",
    "\"endTuple0\",\n",
    "\"endTuple1\",\n",
    "\"endTuple2\",\n",
    "\"verbBeforeNoun\",\n",
    "\"qMark\",\n",
    "\"qVerbCombo\",\n",
    "\"qTripleScore\",\n",
    "\"sTripleScore\",\n",
    "\"class\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "next(reader)  #Assume we have a header \n",
    "for line in reader:\n",
    "    sentence = line[0]  \n",
    "    c = line[1]        #class-label\n",
    "    id = hashlib.md5(str(sentence).encode('utf-8')).hexdigest()[:16] # generate a unique ID\n",
    "    \n",
    "    f = features.features_dict(id,sentence, c)\n",
    "    row = []\n",
    "    \n",
    "    for key in keys:\n",
    "        value = f[key]\n",
    "        row.append(value)\n",
    "    rows.append(row)\n",
    "    \n",
    "faq = pd.DataFrame(rows, columns=keys)\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the Class of Sentence with Previously Built Model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict against FAQ test set\n",
    "featureNames = faq.columns[1:width-1]  #remove the first ID col and last col=classifier\n",
    "faqPreds = clf.predict(faq[featureNames])\n",
    "\n",
    "predout = pd.DataFrame({ 'id' : faq['id'], 'predicted' : faqPreds, 'actual' : faq['class'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Check Accuracy ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-check accuracy ##\n",
    "print(pd.crosstab(faq['class'], faqPreds, rownames=['actual'], colnames=['preds']))\n",
    "\n",
    "print(\"\\n\",pd.crosstab(faq['class'], faqPreds, rownames=['actual'],\n",
    "                       colnames=['preds']).apply(lambda r: round(r/r.sum()*100,2), axis=1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score:\", round(accuracy_score(faq['class'], faqPreds) ,3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be summarised as \"OK\" but not great ...  \n",
    "\n",
    "The Question and Statement predictions are reported as greater than 80% accurate and the features extraction method could easily be expanded on and enhanced.  \n",
    "Also the training data-set is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ad-hoc testing and experiments ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textout = {'Q': \"QUESTION\", 'C': \"CHAT\", 'S':\"STATEMENT\"}\n",
    "\n",
    "mySentence = \"Scikit-learn is a popular Python library for Machine Learning.\"\n",
    "#mySentence = \"The cat is dead\"\n",
    "#mySentence = \"Is the cat dead\"\n",
    "\n",
    "myFeatures = features.features_dict('1',mySentence, 'X')\n",
    "\n",
    "values=[]\n",
    "for key in keys:\n",
    "    values.append(myFeatures[key])\n",
    "\n",
    "s = pd.Series(values)\n",
    "width = len(s)\n",
    "myFeatures = s[1:width-1]  #All but the last item (this is the class for supervised learning mode)\n",
    "predict = clf.predict([myFeatures])\n",
    "\n",
    "print(\"\\n\\nPrediction is: \", textout[predict[0].strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
