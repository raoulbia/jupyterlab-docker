{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Grammar Parsing #\n",
    "\n",
    "To go beyond classifying indivdual words in a sentence and summarising counts and frequencies of items, we can consider the grammer structure of sentences.\n",
    "\n",
    "Using formal grammar to describe the structure of Natural Language with its huge amount of structure variation and word-combinations is very challenging with fixed programmatic approaches.\n",
    "\n",
    "The Context Free Grammar (CFG) is one approach for tackling the challenge of defining relationships and heirarchies between words in a sentence.  *\"A context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the \"block structure\" of sentences in a natural way\"* (Wikepedia - https://en.wikipedia.org/wiki/Context-free_grammar )\n",
    "\n",
    "As well as the core Python NLTK library, there is a widely used package called spaCy, which is written in C and Python: https://spacy.io    Another well known NLP toolkit is the Stanford CoreNLP package. For more advanced use-cases, it is worth exploring some of these other packages as well as NLTK.\n",
    "\n",
    "## Notation and Overview ##\n",
    "Typically the notation convention is to represent Sentences as \"S\".  This can also be a representation of a sub-sentence that can be used to make a bigger sentence - EG, from the Python NLTK Book Chapter 8:\n",
    "\n",
    "*\"If we replaced whole sentences with the symbol S, we would see patterns like Andre said S and I think S. These are templates for taking a sentence and constructing a bigger sentence.\"*\n",
    "\n",
    "Grammars use recursive *productions* of the form S â†’ S and S which build up the meaning of a sentence out of the meanings of its parts.\n",
    "\n",
    "Typical notation for encoding classifications in *Syntactic Categories* is as follows\n",
    "\n",
    "S - Sentence  \n",
    "NP - Noun Phrase  \n",
    "VP - Verb Phrase  \n",
    "NN - Singular Noun \n",
    "PP - Prepositional Phrase\n",
    "DT - Determiner  \n",
    "VI - Intransitive Verb  \n",
    "VT - Transitive Verb  \n",
    "IN - Preposition  \n",
    "\n",
    "\n",
    "## NLTK Grammar CFGs ##\n",
    "\n",
    "The `CFG` class is used to encode grammars.  Each `CFG` consists of a start symbol and a set of productions.\n",
    "The \"start symbol\" specifies the root node value for parse trees which is usually `S`.  \n",
    "\n",
    "One approach is to develop a custom grammar that is suitable for a narrow, specific domain of text processing and save it as a text file.  This text file is then loaded using the CFG.fromstring method and can be used for parsing the sentence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from the NLTK Book, Chapter 8 ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "sentence = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "i=0\n",
    "for tree in parser.parse(sentence):\n",
    "    i=i+1\n",
    "    print(\"Tree\", i)\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.draw()  # alternative NLTK native method\n",
    "import IPython\n",
    "IPython.core.display.display(tree)\n",
    "# (installed Ghostscript on my PC to get this working in Jupyter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the abiguity about the phrase \"in my pajamas\" and whether it describes the elephant or the shooting event.\n",
    "\n",
    "**Observation** on the above: praphrasing the NLTK Book (Chapter 8) - it is *very* hard to scale this approach up for large texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NLP ##\n",
    "\n",
    "The [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/) software package is a very comprehensive, well regarded and actively developed NLP toolset written in *Java*.  Although it is written in Java it is easy to call it from Python and reference processed results.  It is released under the GNU GPL open-source license.\n",
    "\n",
    "Without considering the wider capabilities of the Stanford NLP set of packages, it includes a pre-trained Grammar Dependency Parser that works effectively without having to manually develop your own grammar.\n",
    "\n",
    "Download the Stanford NLP JAR (370 MB) from here:\n",
    "https://stanfordnlp.github.io/CoreNLP/\n",
    "\n",
    "Unzip this to a suitable location for later reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "path = 'D:\\\\SugarSync\\\\Python\\\\stanfordNLP\\\\coreNLP\\\\'   # Set this to where you have downloaded the JAR file to\n",
    "path_to_jar = path + 'stanford-corenlp-3.7.0.jar'\n",
    "path_to_models_jar = path + 'stanford-corenlp-3.7.0-models.jar'\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "os.environ['JAVAHOME'] = 'C:/JAVA32/bin'  # Set this to where the JDK is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 - Dependency Tree Triples ####\n",
    "The parsed result set provides an nltk Dependency Graph object to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = dependency_parser.raw_parse('I shot an elephant in my pajamas')\n",
    "dep = next(result)  # get next item from the iterator result\n",
    "for t in dep.triples():\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background reference for the meaning of the dependency classifications (\"nsubj\", \"nmod\" etc) can be found here: http://universaldependencies.org/u/overview/syntax.html and here: http://universaldependencies.org/u/dep/index.html\n",
    "\n",
    "Find the \"root\" or **\"head word\"** of a phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep.root[\"word\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the **Dependency Tree**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dep.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a **Visulisation** of the dependency tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep.tree().draw()  # alternative NLTK native method\n",
    "import IPython\n",
    "IPython.core.display.display(dep.tree())\n",
    "# (installed Ghostscript on my PC to get this working in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"He watched the dark eyeslits narrowing with greed till her eyes were green stones\"\n",
    "result = dependency_parser.raw_parse(sentence)\n",
    "dep = next(result)\n",
    "dep.root[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.core.display.display(dep.tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I shot an elephant while I was wearing pyjamas\"\n",
    "result = dependency_parser.raw_parse(sentence)\n",
    "dep = next(result)\n",
    "print(\"Head Word:\", dep.root[\"word\"])\n",
    "IPython.core.display.display(dep.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy **Example** Very simple algorithm for deconstructing sentences:\n",
    "1. Identify the core topic or Head Word\n",
    "2. Find a Subject, note the word\n",
    "3. for the Subject, find Compound Entities, add them to Subject-List\n",
    "4. Find an Object, note the word\n",
    "5. For the Object, find the Compound Entities, add them to the Oject-List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regexpSubj = re.compile(r'subj')\n",
    "regexpObj = re.compile(r'obj')\n",
    "regexNouns = re.compile(\"^N.*|^PR.*\")\n",
    "root = dep.root[\"word\"]\n",
    "\n",
    "# A random selection of sentences with different styles, domains etc\n",
    "sentences = [\"He watched the dark eyeslits narrowing with greed till her eyes were green stones\",\n",
    "             \"When will the Oracle 12.2 database be released?\",\n",
    "             \"Coherence is an in-memory grid cluster for Java code\",\n",
    "             \"Oracle 12.2 will be released in March 2017\",\n",
    "             \"PyData community gathers to discuss how best to apply languages and tools to continuously evolving challenges in data management, processing, analytics, and visualization.\",\n",
    "             \"Arsenal are a football team in North London\",\n",
    "             \"When will Arsenal ever win a match?\"]\n",
    "\n",
    "def get_compounds(triples, word):\n",
    "    compound = []\n",
    "    for t in triples:\n",
    "        if t[0][0] == word:\n",
    "            if regexNouns.search(t[2][1]):\n",
    "                compound.append(t[2][0])\n",
    "    return compound\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    result = dependency_parser.raw_parse(sentence)\n",
    "    dep = next(result)\n",
    "    root = [dep.root[\"word\"]]\n",
    "    root.append(get_compounds(dep.triples(), root))\n",
    "    subj = []\n",
    "    obj = []\n",
    "    \n",
    "    for t in dep.triples():\n",
    "        if regexpSubj.search(t[1]):\n",
    "            subj.append(t[2][0])\n",
    "            subj.append(get_compounds(dep.triples(),t[2][0]))\n",
    "        if regexpObj.search(t[1]):\n",
    "            obj.append(t[2][0])\n",
    "            obj.append(get_compounds(dep.triples(),t[2][0]))\n",
    "    print(\"\\n\",sentence)\n",
    "    print(\"Subject:\",subj, \"\\nTopic:\", root, \"\\nObject:\",obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary ###  \n",
    "\n",
    "In summary, the Grammar Parsing capabilities in Stanford CoreNLP takes us beyond classifying individual words in isolation. \n",
    "\n",
    "It provides a method to programmatically analyse sentences by looking at their grammar structure and pick out the \"root\" topic as well as classifying groups of words based on how they form the sentence (Subjects and Objects etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Investigation ##\n",
    "\n",
    "Develop an improved system for analysing the dependency tree for sentence.  Test the accuracy against a set of sentences scraped from the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
